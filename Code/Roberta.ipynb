{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18850,
     "status": "ok",
     "timestamp": 1693786533412,
     "user": {
      "displayName": "Aboli Sahebrao Virkhare",
      "userId": "17059265725778781696"
     },
     "user_tz": -60
    },
    "id": "u3jLs6UKbUyj",
    "outputId": "8a868b5c-b611-45a2-ee19-6c747a4bfba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17606,
     "status": "ok",
     "timestamp": 1693281871820,
     "user": {
      "displayName": "Aboli Sahebrao Virkhare",
      "userId": "17059265725778781696"
     },
     "user_tz": -60
    },
    "id": "ohwBesBsdn9L",
    "outputId": "3e487571-5492-498a-cd2c-d821a5fafa70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7Ei5i8pdA9Z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Device Selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the RoBERTa tokenizer.\n",
    "print('Loading RoBERTa tokenizer...')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "train_df_one = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2013train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "train_df_two = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "train_df_three = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2015train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "train_df_four = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2014sarcasm-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "val_df_one = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2013dev-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "val_df_two = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016devtest-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n",
    "\n",
    "train_data_df = [train_df_one,train_df_two,train_df_three,train_df_four]\n",
    "train_data_df = pd.concat(train_data_df)\n",
    "\n",
    "val_data_df =[val_df_one,val_df_two]\n",
    "val_data_df = pd.concat(val_data_df)\n",
    "\n",
    "train_tweet = train_data_df.tweet.values\n",
    "y_train = train_data_df.label.values\n",
    "\n",
    "val_tweet = val_data_df.tweet.values\n",
    "y_val = val_data_df.label.values\n",
    "\n",
    "train_labels=[]\n",
    "val_labels=[]\n",
    "label_dict = {'negative':0, 'neutral':1, 'positive':2}\n",
    "\n",
    "for label in y_train:\n",
    "  train_labels.append(label_dict[label])\n",
    "\n",
    "for label in y_val:\n",
    "  val_labels.append(label_dict[label])\n",
    "\n",
    "# print(len(train_labels))\n",
    "# print(len(val_labels))\n",
    "\n",
    "def processdata(tweets,labels):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  for tweet in tweets:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels)\n",
    "  return input_ids,attention_masks,labels\n",
    "\n",
    "train_input_ids,train_attention_masks,train_labels = processdata(train_tweet,train_labels)\n",
    "val_input_ids,val_attention_masks,val_labels = processdata(val_tweet,val_labels)\n",
    "\n",
    "print(len(val_input_ids))\n",
    "print(len(val_attention_masks))\n",
    "print(len(val_labels))\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "len(train_dataloader)\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Create the BERT model for sequence classification.\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3,  # The number of output labels = 3\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
    "EPOCHS = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "training_stats=[]\n",
    "def train(model, train_loader, val_loader, optimizer,scheduler):\n",
    "  total_step = len(train_loader)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    # Measure how long the training epoch takes.\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # Reset the total loss and accuracy for this epoch.\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, y) in enumerate(train_loader):\n",
    "\n",
    "      # Unpack this training batch from our dataloader.\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "\n",
    "      #clear any previously calculated gradients before performing a backward pass\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      #Get the loss and prediction\n",
    "      loss, prediction = model(pair_token_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=mask_ids,\n",
    "                             labels=labels).values()\n",
    "\n",
    "      acc = accuracy(prediction, labels)\n",
    "\n",
    "      # Accumulate the training loss and accuracy over all of the batches so that we can\n",
    "      # calculate the average loss at the end\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      # Perform a backward pass to calculate the gradients.\n",
    "      loss.backward()\n",
    "\n",
    "      # Clip the norm of the gradients to 1.0.\n",
    "      # This is to help prevent the \"exploding gradients\" problem.\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Update parameters and take a step using the computed gradient.\n",
    "      optimizer.step()\n",
    "\n",
    "      # Update the learning rate.\n",
    "      scheduler.step()\n",
    "\n",
    "    # Calculate the average accuracy and loss over all of the batches.\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    train_end = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    val_start = time.time()\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, y) in enumerate(val_loader):\n",
    "\n",
    "        #clear any previously calculated gradients before performing a backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Unpack this validation batch from our dataloader.\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        #Get the loss and prediction\n",
    "        loss, prediction = model(pair_token_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=mask_ids,\n",
    "                             labels=labels).values()\n",
    "\n",
    "        # Calculate the accuracy for this batch\n",
    "        acc = accuracy(prediction, labels)\n",
    "\n",
    "        # Accumulate the validation loss and Accuracy\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    # Calculate the average accuracy and loss over all of the batches.\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "\n",
    "    #end = time.time()\n",
    "    val_end = time.time()\n",
    "    hours, rem = divmod(val_end-train_start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': train_loss,\n",
    "            'Valid. Loss': val_loss,\n",
    "            'Valid. Accur.': val_acc,\n",
    "            'Training Time': train_end-train_start,\n",
    "            'Validation Time': val_end-val_start\n",
    "        }\n",
    "    )\n",
    "\n",
    "train(model, train_dataloader, validation_dataloader, optimizer,scheduler)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGaR0mt8T9dk"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016test-A_final.txt\", delimiter='\\t', header=None, names=['label', 'tweet','id'])\n",
    "# Report the number of sentences.\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/Data/2017_English_final/GOLD/Subtask_A/SemEval2017-task4-test.subtask-A.english.txt\", delimiter='\\t', header=None, names=['id','label', 'tweet'])\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "\n",
    "test_tweet = df.tweet.values\n",
    "y_test = df.label.values\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "labels=[]\n",
    "\n",
    "for label in y_test:\n",
    "  labels.append(label_dict[label])\n",
    "\n",
    "# Process test data\n",
    "input_ids, attention_masks, labels = processdata(test_tweet, labels)\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def test(model,prediction_dataloader):\n",
    "\n",
    "  total_test_acc  = 0\n",
    "  total_F1_Score = 0\n",
    "  total_precision = 0\n",
    "  total_recall = 0\n",
    "\n",
    "  for batch_idx, (pair_token_ids, mask_ids,y) in enumerate(prediction_dataloader):\n",
    "    pair_token_ids = pair_token_ids.to(device)\n",
    "    mask_ids = mask_ids.to(device)\n",
    "    labels = y.to(device)\n",
    "\n",
    "    loss, prediction = model(pair_token_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=mask_ids,\n",
    "                             labels=labels).values()\n",
    "\n",
    "\n",
    "    acc = accuracy(prediction, labels)\n",
    "\n",
    "    f1 = metrics.f1_score(labels.cpu(), torch.argmax(prediction, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "    precision = precision_score(labels.cpu(), torch.argmax(prediction, -1).cpu(),labels=[0, 1, 2], average='macro')\n",
    "    recall = recall_score(labels.cpu(), torch.argmax(prediction, -1).cpu(),labels=[0, 1, 2], average='macro')\n",
    "\n",
    "    total_test_acc  += acc.item()\n",
    "    total_F1_Score += f1\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "\n",
    "  test_acc  = total_test_acc/len(prediction_dataloader)\n",
    "  test_f1 = total_F1_Score/len(prediction_dataloader)\n",
    "  test_precision = total_precision/len(prediction_dataloader)\n",
    "  test_recall = total_recall/len(prediction_dataloader)\n",
    "\n",
    "\n",
    "  print(f'test_acc: {test_acc:.4f}')\n",
    "  print(f'f1 Score: {test_f1:.4f}')\n",
    "  print(f'precision: {test_precision:.4f}')\n",
    "  print(f'recall: {test_recall:.4f}')\n",
    "\n",
    "test(model, prediction_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGvoDAB9dmF1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyME+vi4RFS1Aaf52E5b5OXj",
   "gpuType": "T4",
   "mount_file_id": "1Fz7hev4hglyD22nrzOgOn-JTkqaBeqpx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
