{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hlNan4JJDlK0gQqmiawZF-gBJGGAal8R","authorship_tag":"ABX9TyOs1XohO8hpmIwr9kmZJQWG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","\n","# Download NLTK resources if not already downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Define preprocessing functions\n","def preprocess_text(text):\n","    # Lowercase conversion\n","    text = text.lower()\n","\n","    # URL removal\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Username removal\n","    text = re.sub(r'@\\w+', '', text)\n","\n","    # Hashtag removal\n","    text = re.sub(r'#\\w+', '', text)\n","\n","    # Punctuation removal\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    # Tokenization\n","    words = word_tokenize(text)\n","\n","    # Stop words removal\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    return ' '.join(words)\n","\n","# Load DataSets\n","train_df_one = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2013train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_two = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_three = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2015train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_four = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2014sarcasm-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","# test_data_df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016test-A_final.txt\", delimiter='\\t', header=None, names=['label', 'tweet','id'])\n","test_data_df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/Data/2017_English_final/GOLD/Subtask_A/SemEval2017-task4-test.subtask-A.english.txt\", delimiter='\\t', header=None, names=['id','label', 'tweet'])\n","# Concatenate training dataframes\n","train_data_df = [train_df_one, train_df_two, train_df_three, train_df_four]\n","train_data_df = pd.concat(train_data_df)\n","\n","# Preprocess the tweets\n","train_data_df['tweet'] = train_data_df['tweet'].apply(preprocess_text)\n","\n","train_tweet = train_data_df.tweet.values\n","y_train = train_data_df.label.values\n","\n","test_tweet = test_data_df.tweet.values\n","y_test = test_data_df.label.values\n","\n","# Change labels to numeric values\n","train_labels = []\n","test_labels = []\n","label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n","\n","for label in y_train:\n","    train_labels.append(label_dict[label])\n","\n","for label in y_test:\n","    test_labels.append(label_dict[label])\n","\n","print(\"We have {} training samples\".format(len(train_tweet)))\n","print(\"We have {} test samples\".format(len(test_tweet)))\n","\n","# Use TfidfVectorizer to convert text to TF-IDF features\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_tweet)\n","X_test_tfidf = tfidf_vectorizer.transform(test_tweet)\n","\n","# Train the SVM model with regularization (C parameter)\n","svm_classifier = SVC(kernel='linear', C=1.0)  # You can adjust the C value for regularization\n","svm_classifier.fit(X_train_tfidf, train_labels)\n","\n","# Predict the Output\n","predicted = svm_classifier.predict(X_test_tfidf)\n","\n","from sklearn import metrics\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","\n","# Find the Accuracy, precision, recall, and F1 score\n","test_acc = accuracy_score(test_labels, predicted)\n","test_f1 = metrics.f1_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","test_precision = precision_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","test_recall = recall_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","\n","print(f'test_acc: {test_acc:.4f}')\n","print(f'f1 Score: {test_f1:.4f}')\n","print(f'precision: {test_precision:.4f}')\n","print(f'recall: {test_recall:.4f}')\n"],"metadata":{"id":"P7IH4Ef7CD-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FZWu0nZXE1aE"},"execution_count":null,"outputs":[]}]}