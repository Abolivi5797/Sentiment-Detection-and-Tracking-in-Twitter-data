{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1r7lPHfZVDlKtP9p2UqkZbMHyzgRCO2k4","authorship_tag":"ABX9TyMsQAou+vpRnsVgEe2lUpNa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-Mw3OEokoAk5"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"os8l-1zqoEWk","executionInfo":{"status":"ok","timestamp":1694546993585,"user_tz":-60,"elapsed":13189,"user":{"displayName":"Aboli Sahebrao Virkhare","userId":"17059265725778781696"}},"outputId":"b547a333-f2d0-4f1c-b2ab-6dca8ecd9a6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","from sklearn.model_selection import GridSearchCV\n","\n","# Download NLTK resources if not already downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Define preprocessing functions\n","def preprocess_text(text):\n","    # Lowercase conversion\n","    text = text.lower()\n","\n","    # URL removal\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Username removal\n","    text = re.sub(r'@\\w+', '', text)\n","\n","    # Hashtag removal\n","    text = re.sub(r'#\\w+', '', text)\n","\n","    # Punctuation removal\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    # Tokenization\n","    words = word_tokenize(text)\n","\n","    # Stop words removal\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    return ' '.join(words)\n","\n","# Load DataSets\n","train_df_one = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2013train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_two = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_three = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2015train-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","train_df_four = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2014sarcasm-A.txt\", delimiter='\\t', header=None, names=['id', 'label', 'tweet'])\n","test_data_df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/SemEval-2017-Task-4-A-B-C-using-BERT-main/SemEval-2017-Task-4-A-B-C-using-BERT-main/data/twitter-2016test-A_final.txt\", delimiter='\\t', header=None, names=['label', 'tweet','id'])\n","# test_data_df = pd.read_csv(\"/content/drive/MyDrive/Sentiment_Detection/Data/2017_English_final/GOLD/Subtask_A/SemEval2017-task4-test.subtask-A.english.txt\", delimiter='\\t', header=None, names=['id','label', 'tweet'])\n","# Concatenate training dataframes\n","train_data_df = [train_df_one, train_df_two, train_df_three, train_df_four]\n","train_data_df = pd.concat(train_data_df)\n","\n","# Preprocess the tweets\n","train_data_df['tweet'] = train_data_df['tweet'].apply(preprocess_text)\n","\n","train_tweet = train_data_df.tweet.values\n","y_train = train_data_df.label.values\n","\n","test_tweet = test_data_df.tweet.values\n","y_test = test_data_df.label.values\n","\n","# Change labels to numeric values\n","train_labels = []\n","test_labels = []\n","label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n","\n","for label in y_train:\n","    train_labels.append(label_dict[label])\n","\n","for label in y_test:\n","    test_labels.append(label_dict[label])\n","\n","# print(\"We have {} training samples\".format(len(train_tweet)))\n","# print(\"We have {} test samples\".format(len(test_tweet)))\n","\n","# Use TfidfVectorizer to convert text to TF-IDF features\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(train_tweet)\n","X_test_tfidf = tfidf_vectorizer.transform(test_tweet)\n","\n","# Define hyperparameter grid for SVM\n","svm_param_grid = {\n","    'C': [0.1, 10],            # Regularization parameter\n","    'kernel': ['linear'],  # Kernel type\n","}\n","\n","# Create an SVM classifier\n","svm_classifier = SVC()\n","\n","# Perform grid search with cross-validation for SVM\n","svm_grid_search = GridSearchCV(svm_classifier, svm_param_grid, cv=5)\n","svm_grid_search.fit(X_train_tfidf, train_labels)\n","\n","# Print the best hyperparameters for SVM\n","print(\"Best Hyperparameters for SVM:\", svm_grid_search.best_params_)\n","\n","# Get the best SVM model\n","best_svm_model = svm_grid_search.best_estimator_\n","\n","# Predict the Output\n","predicted = best_svm_model.predict(X_test_tfidf)\n","\n","from sklearn import metrics\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","\n","# Find the Accuracy, precision, recall, and F1 score\n","test_acc = accuracy_score(test_labels, predicted)\n","test_f1 = metrics.f1_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","test_precision = precision_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","test_recall = recall_score(test_labels, predicted, labels=[0, 1, 2], average='macro', zero_division=1)\n","\n","print(f'test_acc: {test_acc:.4f}')\n","print(f'f1 Score: {test_f1:.4f}')\n","print(f'precision: {test_precision:.4f}')\n","print(f'recall: {test_recall:.4f}')\n"],"metadata":{"id":"Y3fh_6SBCaep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XPYN_sUVI32w"},"execution_count":null,"outputs":[]}]}